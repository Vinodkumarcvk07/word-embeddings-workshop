{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Creating Word Embeddings with Brian Spiering\n",
    "-------\n",
    "<center><img src=\"https://imgs.xkcd.com/comics/machine_learning.png\" width=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Who am I?\n",
    "-----\n",
    "\n",
    "<center><img src=\"images/profile.JPG\" width=\"500\"/></center>\n",
    "\n",
    "Dr. Brian Spiering \n",
    "\n",
    "Computer Science (CS) faculty (new-ish)  \n",
    "Natural Language Processing (NLP) and Artificial Intelligence (AI) (mostly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If you want to follow along with these slides…\n",
    "------\n",
    "\n",
    "[bit.ly/word-workshop](http://bit.ly/word-workshop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "By The End Of This Session You Should Be Able To:\n",
    "---\n",
    "\n",
    "- Describe why word embeddings are a popular and powerful technical\n",
    "- Explain how word embeddings is a neural network\n",
    "- Understand the common architectures of word embeddings\n",
    "- Apply word embeddings to variety of problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "5 Is of Technology: Word Embeddings\n",
    "-------\n",
    "\n",
    "1. Information - What is it?\n",
    "1. Inspiration - Why is it important?\n",
    "1. Install - How do I set it up?\n",
    "1. Implementation - How do I use it?\n",
    "1. Integration - How does it become automatic?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Pop Quiz\n",
    "---\n",
    "\n",
    "Do computers prefer numbers or words?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__Numbers__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "1. Information: What are Word Embeddings?\n",
    "-----\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "Word Embedding are a collection of algorithms that map words (strings) to numbers (vectors - lists of floats).\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "2. Inspiration - Why is it important?\n",
    "-----\n",
    "\n",
    "Turns text into a numerical form (meaningful word vectors) allows computers to better process the information.\n",
    "\n",
    "Including creating a representation that other Deep Learning and Machine Learning algorithms can use in-turn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How does word2vec work?\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/firth.png\" width=\"400\"/></center>\n",
    "\n",
    ">“You shall know a word\n",
    ">by the company it keeps”\n",
    "\n",
    "> \\- J. R. Firth 1957"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Distributional Hypothesis\n",
    "----\n",
    "\n",
    "> Words that are used and occur in the same contexts tend to have similar meanings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__Example:__  \n",
    "> ... government debt problems are turning into __banking__ crises...  \n",
    "\n",
    "> ... Europe governments needs unified __banking__ regulation to replace the hodgepodge of debt regulations...\n",
    "\n",
    "The words: _government_, _regulation_ and _debt_ probably represent some aspect of _banking_ since they frequently appear near by.\n",
    "\n",
    "The words: _Pokeman_ and _tublar_ probably do __not__ represent some aspect of _banking_ since they don't frequently appear near by."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How does word2vec model the Distributional Hypothesis?\n",
    "---\n",
    "\n",
    "word2Vec is a very simple neural network:\n",
    "\n",
    "<center><img src=\"images/w2v_neural_net.png\" width=\"700\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "\n",
    "[Source](http://www-personal.umich.edu/~ronxin/pdf/w2vexp.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A Simplified version of word2vec\n",
    "------\n",
    "\n",
    "<center><img src=\"http://adventuresinmachinelearning.com/wp-content/uploads/2017/07/Word2Vec-softmax.jpg\" width=\"700\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Once the training is complete, the output softmax layer is discarded and the remaining weights become the embedding layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "word2vec as a compression algorithm\n",
    "-----\n",
    "\n",
    "<center><img src=\"images/w2v_neural_net.png\" width=\"700\"/></center>\n",
    "\n",
    "Note the bow-tie shape - that is is an __autoencoder__. \n",
    "\n",
    "Autoencoders compress a sparse representation into a dense representation. They learns the mapping that best preserves the structure of the original space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Story time...\n",
    "----\n",
    "\n",
    "<center><img src=\"http://worldartsme.com/images/king-and-queen-clipart-1.jpg\" width=\"300\"/></center>\n",
    "\n",
    "A man and woman meet each other ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The man and woman become king and queen ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The king and queen get old and stop talking to each other. Instead, they read books and magazines ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Represent data\n",
    "corpus = \"\"\"The man and woman meet each other ...\n",
    "         The man and woman become king and queen ...\n",
    "         The king and queen get old and stop talking to each other. Instead, they read books and magazines ...\n",
    "         \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-78b1325757b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mimportant_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'queen'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'book'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'king'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'magazine'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'woman'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'man'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m vectors = np.array([[0.1,   0.3],  # queen\n\u001b[0m\u001b[1;32m      5\u001b[0m                     \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# book\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                     \u001b[0;34m[\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m   \u001b[0;36m0.2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# king\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# Assign important words to vectors by hand\n",
    "important_words = ['queen', 'book', 'king', 'magazine', 'woman', 'man']\n",
    "\n",
    "vectors = np.array([[0.1,   0.3],  # queen\n",
    "                    [-0.5, -0.1],  # book\n",
    "                    [0.2,   0.2],  # king\n",
    "                    [-0.3, -0.2],  # magazine\n",
    "                    [-0.5,  0.4],  # car\n",
    "                    [-0.45, 0.3]]) # bike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the most important words\n",
    "plt.plot(vectors[:,0], vectors[:,1], 'o')\n",
    "for word, x, y in zip(important_words, vectors[:,0], vector®s[:,1]):\n",
    "    plt.annotate(word, (x, y), size=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "---\n",
    "\n",
    "How many dimensions are data represented in? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "There are 2 dimensions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How many dimensions would we need to represent for naive word vectors? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__5 dimensions__\n",
    "\n",
    "Typically you would use n-1 word vectors (a baseline word would be coded as all zeros). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "```python\n",
    "# Encode each word using 1-hot encoding\n",
    "{'queen':    [0, 0, 0, 0, 0],\n",
    " 'book':     [0, 0, 0, 0, 1],\n",
    " 'king':     [0, 0, 0, 1, 0],\n",
    " 'magazine': [0, 0, 1, 0, 0],\n",
    " 'woman':    [0, 1, 0, 0, 0],\n",
    " 'man':      [1, 0, 0, 0, 0],\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "word2vec creates low-dimensional, dense vectors\n",
    "====\n",
    "\n",
    "In contrast to other NLP encodings which are larger and sparser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In terms of Big O space complexity, how does word2vec vs 1-hot encoding scale?\n",
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "word2vec is constant O(1)\n",
    "\n",
    "1-hot encoding is linear O(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What are the differences between the representations of images or sound compared to text?\n",
    "--------\n",
    "<center><img src=\"images/representation.png\" width=\"700\"/></center>\n",
    "<center><img src=\"images/doc_matrix.png\" width=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/w2v_neural_net.png\" width=\"700\"/></center>\n",
    "\n",
    "The vectors are the weights in the neural network. Each hidden node is a dimension.\n",
    "\n",
    "The dimensions are not directly interpretable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The 2 architectures of word2vec training\n",
    "----\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1) “Skip-gram”: Predict target word, given a nearby word\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "2) “Continuous bag of words”: Predict target word, given a context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Sentence:  \n",
    "\"Selling these fine leather jackets\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Skip-gram architecture: <br> Predict target word, given a nearby word\n",
    "----\n",
    "\n",
    "<center><img src=\"images/skip_gram2.png\" width=\"600\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "Skip-gram example\n",
    "---\n",
    "\n",
    ">“… selling these fine leather jackets”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```\n",
    "bi-grams = {selling these, these fine, \n",
    "            fine leather, leather jackets}\n",
    "\n",
    "skip-two-bi-grams = {selling these, selling fine, selling fine, \n",
    "                    these fine, these fine, these jackets, \n",
    "                    fine leather, fine jackets, \n",
    "                    leather jackets}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Defining skip-grams\n",
    "---\n",
    "\n",
    "<img src=\"images/skip-gram-equation.png\" style=\"width: 400px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Skip-Gram architecture, deep dive\n",
    "----\n",
    "\n",
    "<img src=\"images/skip_gram_detailed.png\" style=\"width: 400px;\"/>\n",
    "\n",
    "The target word is now at the input layer, and the context words are on the output layer.\n",
    "\n",
    "On the output layer, instead of outputing one multinomial distribution, we are outputing C multinomial distributions. Each output is computed using the same hidden to output matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Continuous bag of words (CBOW) architecture: <br>Predict target word, given a context\n",
    "-----\n",
    "\n",
    "<center><img src=\"images/cbow2.png\" width=\"500\"/></center>\n",
    "\n",
    "The context is represented as a bag of the words contained in a fixed size window around the target word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Skip-gram vs. CBOW\n",
    "-----\n",
    "\n",
    "<center><img src=\"images/cbo_vs_skipgram.png\" width=\"700\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Reference](https://fasttext.cc/docs/en/unsupervised-tutorial.html)\n",
    "\n",
    "[Detailed explanation](http://alexminnaar.com/word2vec-tutorial-part-ii-the-continuous-bag-of-words-model.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "CBOW vs. Skip-gram\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "CBOW is several times faster to train than the skip-gram and has slightly better accuracy for  frequent words.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Skip-gram works well with a small amount of the training data and well represents rare words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__Skip-gram is the most common architecture.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Summary\n",
    "------\n",
    "\n",
    "<center><img src=\"images/book.png\" width=\"700\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Summary\n",
    "----\n",
    "\n",
    "- Word2vec: Create a dense vector representation of words that models semantic meaning based on context\n",
    "- Word2Vec is popular because it is straight forward to implement and creates dense embedding vectors.\n",
    "- Word2Vec is a _relatively_ simple neural net with 1 input layer, 1 hidden layer, and 1 output layer.\n",
    "- There are 2 common architectures: \n",
    "    1. CBOW: given context, predict word\n",
    "    2. skip-gram: given word, predict context\n",
    "- Sets you up for machine learning and Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br>\n",
    "<br> \n",
    "<br>\n",
    "\n",
    "----"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
